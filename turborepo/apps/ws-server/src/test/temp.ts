import { Fs } from "@d0paminedriven/fs";

const obj = {
  type: "ai_chat_response",
  conversationId: "abc123",
  userId: "x1sa9esbc7nb1bbhnn5uy9ct",
  provider: "openai",
  model: "gpt-4.1-nano-2025-04-14",
  chunk:
    "Certainly! To implement a `request_ai_chat` server action in Next.js 15 with TypeScript and Prisma, you'll typically want to:\n\n1. **Define the Prisma schema** for your chat messages and any relevant data.\n2. **Create an API route or server action** that handles the `request_ai_chat` event, which:\n   - Receives user input (like the message content).\n   - Stores the message in the database.\n   - Invokes your LLM API (e.g., OpenAI) to get a response.\n   - Stores the AI response.\n   - Sends back the reply to the frontend.\n\nHere's an example implementation:\n\n---\n\n### 1. Prisma Schema (schema.prisma)\n\nMake sure your Prisma models are set up:\n\n```prisma\nmodel User {\n  id        Int      @id @default(autoincrement())\n  email     String   @unique\n  name      String?\n  messages  Message[]\n  // other fields...\n}\n\nmodel Message {\n  id        Int      @id @default(autoincrement())\n  content   String\n  senderId  Int\n  sender    User     @relation(fields: [senderId], references: [id])\n  createdAt DateTime @default(now())\n}\n```\n\nRun `npx prisma migrate dev` after setting this up.\n\n---\n\n### 2. API Route (app/api/request_ai_chat/route.ts)\n\nCreate a server action in Next.js 15:\n\n```typescript\nimport { NextResponse } from 'next/server'\nimport { PrismaClient } from '@prisma/client'\n\n// Initialize Prisma Client\nconst prisma = new PrismaClient()\n\n// You might have environment variables for OpenAI API key\nconst OPENAI_API_KEY = process.env.OPENAI_API_KEY\n\n// Function to call OpenAI API (or your LLM)\nasync function fetchAIResponse(prompt: string): Promise<string> {\n  const response = await fetch('https://api.openai.com/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${OPENAI_API_KEY}`,\n    },\n    body: JSON.stringify({\n      model: 'gpt-3.5-turbo',\n      messages: [{ role: 'user', content: prompt }],\n      max_tokens: 150,\n    }),\n  })\n\n  if (!response.ok) {\n    throw new Error('Failed to fetch AI response')\n  }\n\n  const data = await response.json()\n  const message = data.choices[0].message.content\n  return message\n}\n\n// Server action to handle 'request_ai_chat' event\nexport async function POST(request: Request) {\n  try {\n    const body = await request.json()\n    \n    const { userId, messageContent } = body\n\n    if (!userId || !messageContent) {\n      return NextResponse.json({ error: 'Invalid input' }, { status: 400 })\n    }\n\n    // Save user message\n    const userMessage = await prisma.message.create({\n      data: {\n        content: messageContent,\n        senderId: userId,\n      },\n    })\n\n    // Call AI service\n    const aiResponse = await fetchAIResponse(messageContent)\n\n    // Save AI response as message\n    const aiMessage = await prisma.message.create({\n      data: {\n        content: aiResponse,\n        senderId: null, // or assign to \"AI\" user if you have one\n      },\n    })\n\n    // Return the AI message\n    return NextResponse.json({\n      userMessage,\n      aiMessage,\n    })\n  } catch (err) {\n    console.error(err)\n    return NextResponse.json({ error: 'Internal Server Error' }, { status: 500 })\n  }\n}\n```\n\n### 3. Frontend Usage (e.g., in a React component)\n\n/** Example: handling submit and invoking the server action **/\n\n```tsx\nimport { useState } from 'react'\n\nfunction ChatInput({ userId }: { userId: number }) {\n  const [input, setInput] = useState('')\n  const [messages, setMessages] = useState<any[]>([])\n\n  const handleSubmit = async () => {\n    if (!input.trim()) return\n\n    // Call API route\n    const response = await fetch('/api/request_ai_chat', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ userId, messageContent: input }),\n    })\n\n    if (response.ok) {\n      const data = await response.json()\n      // Append new messages to chat\n      setMessages(prev => [...prev, data.userMessage, data.aiMessage])\n      setInput('')\n    } else {\n      console.error('Failed to request AI chat')\n    }\n  }\n\n  return (\n    <div>\n      <div>\n        {messages.map((msg, index) => (\n          <div key={index}>{msg.content}</div>\n        ))}\n      </div>\n      <input\n        value={input}\n        onChange={(e) => setInput(e.target.value)}\n        placeholder=\"Type your message\"\n      />\n      <button onClick={handleSubmit}>Send</button>\n    </div>\n  )\n}\n```\n\n---\n\n### Summary:\n- You set up Prisma models to store messages.\n- Create a Next.js server action (`route.ts`) to handle chat requests:\n  - Save user message.\n  - Call LLM API.\n  - Save AI response.\n  - Return the response.\n- Call this API from your React component when user submits a message.\n\n---\n\n**Note:** Make sure to have your OpenAI API key in environment variables (`.env`) and handle any errors or edge cases as needed. Adjust the Prisma models and data flow to fit your app's architecture.\n\nLet me know if you'd like a more detailed setup or specific parts!",
  done: true
};
const fs = new Fs(process.cwd());

fs.withWs("src/__out__/openai/request-ai-chat-writeup.md", obj.chunk);
